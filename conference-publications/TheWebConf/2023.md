# [Publications in TheWebConf 2023](https://dl.acm.org/doi/proceedings/10.1145/3543507?tocHeading=heading14)

 - [Graph Self-supervised Learning](#graph-self-supervised-learning)
 - [General Contrastive Learning](#general-contrastive-learning)
 - [Weakly-supervised Learning](#weakly-supervised-learning)
 - [Label Noise Learning](#label-noise-learning)
   
## Graph Self-supervised Learning
### Expressive and Eficient Representation Learning for Ranking Links in Temporal Graphs
 - [[paper](https://dl.acm.org/doi/pdf/10.1145/3543507.3583476)]
 - [[code](https://github.com/susheels/tgrank)]
 - 时间图表示学习 （T-GRL） 旨在学习对图边如何随时间演变进行建模的表征。虽然最近关于T-GRL的工作提高了时间设置下的链路预测精度，但他们的方法在未来的链路上独立地优化逐点损失函数，而不是在每个节点的候选集上联合优化。在资源（例如，注意力）根据可能性的排名链接分配的应用程序中，最好使用排名损失。然而，开发一种 T-GRL 方法来优化由于模型表现力和可扩展性之间的权衡而导致的排名损失并不简单。在这项工作中，我们解决了这些问题，并提出了一个用于排名的时间图网络（TGRank），它通过（i）优化列表损失以提高排名，以及（ii）结合一种标记方法，旨在允许对候选集进行有效的推理，同时证明提高表达能力，从而显着提高了链接预测任务的性能。我们在六个真实网络上广泛评估了TGRank。TGRank 在排名指标方面平均比最先进的基线高出 14.21%↑（转导）和 16.25% ↑（归纳），同时在大型网络上进行推理的效率更高（高达 65× 加速）。

### INCREASE: Inductive Graph Representation Learning for Spatio-Temporal Kriging
 - [[paper](https://dl.acm.org/doi/pdf/10.1145/3543507.3583525)]
 - [[code](https://github.com/zhengchuanpan/INCREASE)]
 - 时空克里金法是网络和社会应用（如网络或物联网）中的一个重要问题，其中连接到网络的事物（例如传感器）通常具有空间和时间属性。它旨在使用在给定感兴趣的时间段内来自（事物）观察到的位置的数据来推断未观察到的位置（事物）的知识。这个问题本质上需要归纳学习。训练后，模型应该能够对不同的位置（包括新给定的位置）执行克里金法，而无需重新训练。然而，由于空间关系的异质性和时间模式的多样性，很难获得准确的克里金法结果。在本文中，我们提出了一种新的时空克里金法归纳图表示学习模型。首先，通过空间接近性、功能相似性和转移概率对未观测位置和观测位置之间的异构空间关系进行编码。基于每种关系，我们通过共同建模未观测位置的相似性和差异性，准确地汇总了最相关的观测位置的信息，从而为未观测到的位置生成归纳表示。然后，我们设计了关系感知门控递归单元（GRU）网络，以自适应地捕获每个关系生成的序列表示中的时间相关性。最后，提出了一种多关系注意力机制，将来自多个关系的不同时间步长的复杂时空信息动态融合，以计算克里金输出。在三个真实世界数据集上的实验结果表明，我们提出的模型始终优于最先进的方法，并且在观测位置较少时，优势更为明显。

### Automated Spatio-Temporal Graph Contrastive Learning
 - [[paper](https://dl.acm.org/doi/pdf/10.1145/3543507.3583304)]
 - [[code](https://github.com/HKUDS/AutoST)]
 - 在各种区域嵌入方法中，基于图的区域关系学习模型具有较强的结构表示能力，能够利用图神经网络对空间相关性进行编码。尽管它们很有效，但现有方法中有几个关键挑战尚未得到很好的解决：i）由于各种因素，数据噪声和丢失在许多时空场景中无处不在。ii） 输入的时空数据（例如，移动迹线）通常表现出跨空间和时间的分布异质性。在这种情况下，当前的方法容易受到生成的区域图质量的影响，这可能会导致性能欠佳。在本文中，我们通过探索基于多视图数据源生成的异构区域图的自动时空图对比学习范式（AutoST）来解决上述挑战。我们的 AutoST 框架建立在异构图神经架构之上，以捕获与 POI 语义、移动模式和地理位置相关的多视图区域依赖关系。为了提高我们的GNN编码器对数据噪声和分布问题的鲁棒性，我们设计了一种带有参数化对比视图生成器的自动时空增强方案。AutoST可以适配时空异构图，多视图语义保留得当。在几个真实世界的数据集上对三个下游时空挖掘任务进行了广泛的实验，证明了我们的AutoST在各种基线上实现了显著的性能提升。

### CitationSum: Citation-aware Graph Contrastive Learning for Scientific Paper Summarization
 - [[paper](https://dl.acm.org/doi/pdf/10.1145/3543507.3583505)]
 - [[code](https://github.com/zhehengluoK/CitationSum)]
 - 引文图有助于生成高质量的科学论文摘要，其中科学论文的参考文献及其相关性可以提供额外的知识，以了解其背景和主要贡献。尽管引文图的贡献很有希望，但将它们纳入摘要任务仍然具有挑战性。这是由于难以准确识别和利用源论文参考文献中的相关内容，以及捕获它们不同强度的相关性。现有的方法要么忽略参考文献，要么只不分青红皂白地使用参考文献的摘要，无法解决上述挑战。为了填补这一空白，我们提出了一种基于引文图的新型引文感知科学论文摘要框架，能够准确地定位和整合参考文献中的突出内容，并捕获源论文与其参考文献之间的不同相关性。具体来说，我们首先建立了一个特定领域的数据集 PubMedCite，其中包含大约 192K 篇生物医学科学论文和一个大型引文图，保留了它们之间的 917K 引文关系。其特点是保留了从参考文献全文中提取的突出内容，以及参考文献突出内容与源论文之间的加权相关性。在此基础上，我们设计了一种具有图对比学习的自监督引文感知摘要框架（CitationSum），该框架通过在相关性的指导下将参考文献中的突出信息与源论文内容有效地融合，从而促进摘要生成。实验结果表明，由于有效地利用了参考文献和引文相关性的信息，我们的模型优于最先进的方法。

### Generating Counterfactual Hard Negative Samples for Graph Contrastive Learning
 - [[paper](https://dl.acm.org/doi/pdf/10.1145/3543507.3583499)]
 - [[code](https://www.dropbox.com/sh/kyf8p9unkhn0r99/AABd33jFBfjGYIkvIqWpuNwYa?dl=0)]
 - 图对比学习已成为一种强大的无监督图表示学习工具。图对比学习成功的关键是获取高质量的正负样本作为对比对，以学习输入图的底层结构语义。最近的工作通常从与正样本相同的训练批次或外部不相关的图中抽取负样本。然而，这种策略存在一个显著的局限性：对假阴性样本进行抽样是不可避免的问题。在本文中，我们提出了一种新的方法，利用反事实机制为图对比学习生成人工硬负样本，即CGC。我们利用反事实机制来生成硬负样本，确保生成的样本相似，但具有与正样本不同的标签。所提方法在多个数据集上取得了令人满意的结果。它优于一些传统的无监督图学习方法和一些SOTA图对比学习方法。我们还进行了一些补充实验来说明所提出的方法，包括CGC在不同硬负样本下的性能，以及对不同相似性测量产生的硬负性样本的评估。

### GraphMAE2: A Decoding-Enhanced Masked Self-Supervised Graph Learner
 - [[paper](https://dl.acm.org/doi/pdf/10.1145/3543507.3583379)]
 - [[code](https://github.com/THUDM/GraphMAE2)]
 - 图自监督学习 （SSL），包括对比和生成方法，在解决真实世界图数据中标签稀缺的根本挑战方面具有巨大潜力。在这两组图SSL技术中，掩码图自编码器（例如GraphMAE）——一种生成方法——最近产生了有希望的结果。这背后的想法是用自动编码器架构重建节点特征（或结构），这些特征（或结构）从输入中随机屏蔽。然而，掩码特征重建的性能自然依赖于输入特征的可区分性，并且通常容易受到特征的干扰。在本文中，我们提出了一个掩蔽的自监督学习框架1 GraphMAE2，目的是克服这个问题。这个想法是对图 SSL 的特征重建施加正则化。具体而言，我们设计了多视图随机重掩码解码和潜在表示预测策略，以正则化特征重构。多视图随机重掩码解码是在特征空间中引入随机性，而潜在表示预测是在嵌入空间中强制重构。大量实验表明，GraphMAE2 可以在各种公共数据集上始终如一地生成最佳结果，包括与具有 111M 节点和 1.6B 边缘的 ogbnPapers100M 上最先进的基线相比至少提高 2.45%。

### Graph Self-supervised Learning with Augmentation-aware Contrastive Learning
 - [[paper](https://dl.acm.org/doi/pdf/10.1145/3543507.3583246)]
 - 图自监督学习旨在从未标记的图数据中挖掘有用的信息，并已成功应用于预训练图表示。许多现有方法使用对比学习，通过从两个增强图视图进行对比学习来学习强大的嵌入。然而，这些图对比方法都没有充分利用不同增强的多样性，因此容易出现过度和有限的学习表示的泛化能力。在本文中，我们提出了一种具有增强感知对比学习的新型图自监督学习方法。该方法基于预训练模型在添加增强多样性后可以获得更好的泛化能力的假设。为了充分利用多样化增强方法的信息，该文构建了一种新的增强感知预测任务，与对比学习任务相辅相成。与预训练需要快速适应不同的下游任务类似，我们对构建的任务进行模拟训练测试适应，以进一步增强学习能力;这种策略可以被视为元学习的一种形式。实验结果表明，该方法优于以往方法，在各种下游任务中都能更好地学习表示。

### Self-Supervised Teaching and Learning of Representations on Graphs
 - [[paper](https://dl.acm.org/doi/pdf/10.1145/3543507.3583441)]
 - 近年来，图对比学习（GCL）取得了重大进展，而大多数GCL模型使用图神经网络作为基于监督学习的编码器。在这项工作中，我们提出了一种名为GraphTL的新颖图学习模型，该模型探索了图上表示的自监督教学和学习。GCL 的一个关键目标是保留原始图形信息。为此，我们设计了一种基于局部线性嵌入（LLE）的无监督降维思想的编码器。具体来说，我们将LLE的一次迭代映射到网络的一层。为了引导编码器更好地保留原始图信息，我们提出了一个由两个视图组成的非平衡对比模型，分别是学习视图和教学视图。此外，我们将多视图中相同的节点视为正节点对，并设计了节点相似度评分器，以便模型能够选择目标节点的正样本。已经在多个数据集上进行了广泛的实验，以评估 GraphTL 与基线模型相比的性能。结果表明，GraphTL可以减少相似节点之间的距离，同时保留网络拓扑和特征信息，从而在节点分类方面产生更好的性能。

### SeeGera: Self-supervised Semi-implicit Graph Variational Auto-encoders with Masking
 - [[paper](https://dl.acm.org/doi/pdf/10.1145/3543507.3583245)]
 - [[code](https://github.com/SeeGera/SeeGera)]
 - 生成图自监督学习（SSL）旨在通过重构输入图数据来学习节点表示。然而，大多数现有方法只关注无监督学习任务，很少有工作显示出其优于最先进的图对比学习（GCL）模型，特别是在分类任务上。虽然最近已经提出了一个模型来弥合这一差距，但它在无监督学习任务中的表现仍然未知。在本文中，为了全面提高生成图SSL相对于其他GCL模型在无监督和监督学习任务上的性能，我们提出了SeeGera模型，该模型基于自监督变分图自编码器（VGAE）系列。具体而言，SeeGera采用半隐式变分推理框架，即分层变分框架，主要关注特征重构和结构/特征掩码。一方面，SeeGera在编码器中共同嵌入了节点和特征，并在解码器中重建了链接和特征。由于特征嵌入包含丰富的特征语义信息，因此可以与节点嵌入相结合，为特征重构提供粒度知识。另一方面，SeeGera在分层变分框架中增加了一个额外的结构/特征掩蔽层，从而提高了模型的泛化性。我们进行了广泛的实验，将SeeGera与其他9个最先进的竞争对手进行了比较。我们的结果表明，SeeGera在各种无监督和监督学习任务中可以与其他最先进的GCL方法相媲美。

### Automated Self-Supervised Learning for Recommendation
 - [[paper](https://dl.acm.org/doi/pdf/10.1145/3543507.3583336)]
 - [[code](https://github.com/HKUDS/AutoCF)]
 - 图神经网络（GNN）已成为协同过滤（CF）的最新范式。为了提高有限标记数据的表示质量，对比学习在推荐和基于图的CF模型中引起了人们的关注。然而，大多数对比方法的成功很大程度上依赖于手动生成有效的对比视图，以实现基于启发式的数据增强。这不会在不同的数据集和下游推荐任务中泛化，这很难适应数据增强和对噪声扰动的鲁棒性。为了弥补这一关键差距，这项工作提出了一种单馈自动协同过滤（AutoCF）来自动执行数据增强以供推荐。具体来说，我们专注于具有可学习增强范式的生成式自监督学习框架，该范式有利于重要自监督信号的自动蒸馏。为了增强表示判别能力，我们的掩码图自编码器旨在通过重构掩码子图结构来聚合增强过程中的全局信息。在几个公共数据集上进行实验和消融研究，以推荐产品、场所和位置。结果表明，AutoCF优于各种基线方法。

### Hashtag-Guided Low-Resource Tweet Classification
 - [[paper](https://dl.acm.org/doi/pdf/10.1145/3543507.3583194)]
 - [[code](https://github.com/shizhediao/HashTation)]
 - 社交媒体分类任务（例如，推文情绪分析、推文立场检测）具有挑战性，因为社交媒体帖子通常简短、非正式且模棱两可。因此，对推文的训练具有挑战性，并且需要大规模的人工注释标签，这既耗时又昂贵。在本文中，我们发现为社交媒体推文提供主题标签可以帮助缓解这个问题，因为主题标签可以在各种信息方面丰富简短和模棱两可的推文，例如主题、情绪和立场。这促使我们提出了一种新颖的 Hashtagguided Tweet Classifcation 模型 （HashTation），该模型自动为输入推文生成有意义的主题标签，为推文分类提供有用的辅助信号。为了生成高质量和有见地的主题标签，我们的主题标签生成模型检索和编码整个语料库中的后级和实体级信息。实验表明，HashTation在仅提供有限数量的训练数据的七个低资源推文分类任务上取得了显著的改进，这表明使用模型生成的主题标签自动丰富推文可以显着减少对大规模人工标记数据的需求。进一步的分析表明，HashTation能够生成与推文及其标签一致的高质量主题标签。

### GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks
 - [[paper](https://dl.acm.org/doi/pdf/10.1145/3543507.3583386)]
 - 图形可以对对象之间的复杂关系进行建模，从而实现无数的 Web 应用程序，例如在线页面/文章分类和社会推荐。虽然图神经网络 （GNN） 已成为图表示学习的强大工具，但在端到端监督环境中，它们的性能在很大程度上依赖于大量特定于任务的监督。为了减少标签要求，“预训练、fne-tune”和“预训练、提示”范式变得越来越普遍。特别是，提示是自然语言处理中 fne-tuning 的一种流行替代方案，它旨在以特定于任务的方式缩小预训练和下游目标之间的差距。然而，现有的对图提示的研究仍然有限，缺乏一种普遍的治疗方法来吸引不同的下游任务。在本文中，我们提出了GraphPrompt，一种新颖的图预训练和提示框架。GraphPrompt 不仅将预训练和下游任务整合到一个通用的任务模板中，而且还使用可学习的提示来帮助下游任务以特定于任务的方式从预训练模型中查找最相关的知识。最后，我们对五个公共数据集进行了广泛的实验，以评估和分析GraphPrompt。

## General Contrastive Learning

## Weakly-supervised Learning

## Label Noise Learning
