# Publications in ACM MM 2022

## Emotion-Related
### A Multi-view Spectral-Spatial-Temporal Masked Autoencoder for Decoding Emotions with Self-supervised Learning
 - [[paper](https://dl.acm.org/doi/10.1145/3503161.3548243)]
 - 情感脑机接口已经取得了相当大的进步，研究人员可以成功地解释在实验室环境中收集的标记和完美的脑电图数据。然而，脑电数据的标注耗时，需要大量的劳动力，这限制了在实际场景中的应用。此外，由于脑电信号对噪声敏感，每天收集的脑电图数据可能会部分损坏。在本文中，我们提出了一种具有自监督学习功能的多视图光谱-时空掩蔽自编码器（MV-SSTMA），以应对日常应用中的这些挑战。MV-SSTMA基于多视角CNNTransformer混合结构，从频谱、空间和时间角度解读脑电信号的情绪相关知识。我们的模型由三个阶段组成：1）在广义预训练阶段，来自所有受试者的未标记脑电图数据通道被随机屏蔽，然后重建以从脑电图数据中学习通用表示;2）在个性化标定阶段，仅使用来自特定主体的少量标注数据来标定模型;3）在个人测试阶段，我们的模型可以从声音脑电图数据中解码个人情绪，也可以解码缺失通道的受损情绪。在两个开放的情绪脑电数据集上进行的大量实验表明，我们提出的模型在情绪识别方面取得了最先进的性能。此外，在通道缺失的异常情况下，所提模型仍能有效识别情绪。

### SER30K: A Large-Scale Dataset for Sticker Emotion Recognition
 - [[paper](https://dl.acm.org/doi/10.1145/3503161.3548407)]
 - [[code](https://github.com/nku-shengzheliu/SER30K)]
 - 随着即时通讯应用程序的普及，在线聊天在我们的日常生活中起着至关重要的作用。在线聊天中普遍使用贴纸来表达情感，这导致了多模态贴纸情感识别的必要性。考虑到贴图情绪数据的缺乏，我们收集了一个名为SER30K的大规模贴图情绪识别数据集。它由总共 1,887 个贴纸主题组成，总共有 30,739 张贴纸图片。一些常用的图像，如逼真的图像和面部表情图像，在情感分析领域已经得到了很好的研究。然而，理解贴纸图像的情感仍然具有挑战性。由于同一主题的贴纸特征相似，我们只能通过捕捉局部信息（例如表情、姿势）和理解全局信息（例如物体之间的关系）来准确预测情绪。为了应对这一挑战，我们提出了一个LOcal Re-Attention多模态网络（LORA），以端到端的方式学习贴纸情绪。与以前使用卷积神经网络的方法不同，LORA使用视觉转换器来提取视觉特征，从而更好地捕捉全局关系。此外，我们还设计了一个本地重新关注模块，以关注重要的区域信息。然后，一个简单但高效的模态融合模块结合了视觉和语言功能。在SER30K等情绪识别数据集上进行了大量实验，验证了所提方法的有效性。

### Towards Unbiased Visual Emotion Recognition via Causal Intervention
 - [[paper](https://dl.acm.org/doi/10.1145/3503161.3547936)]
 - 尽管在视觉情感识别方面已经取得了很大进展，但研究人员已经意识到，现代深度网络倾向于利用数据集特征来学习输入和目标之间的虚假统计关联。此类数据集特征通常被视为数据集偏差，这会损害这些识别系统的鲁棒性和泛化性能。在这项工作中，我们从因果推理的角度仔细研究了这个问题，其中这种数据集特征被称为混杂因素，它误导系统学习虚假相关性。为了缓解数据集偏差带来的负面影响，我们提出了一种新的介入性情绪识别网络（IERN）来实现后门调整，这是因果推理中的一种基本去混淆技术。具体来说，IERN首先将与数据集相关的上下文特征与实际的情感特征分开，其中前者构成了混杂因素。然后，情感特征将被迫平等地看到每个混杂层，然后再输入分类器。一系列设计的测试验证了 IERN 的功效，对三个情绪基准的实验表明，IERN 在无偏见的视觉情绪识别方面优于最先进的方法。

 ### Unsupervised Domain Adaptation Integrating Transformer and Mutual Information for Cross-Corpus Speech Emotion Recognition
 - [[paper](https://dl.acm.org/doi/10.1145/3503161.3548328)]
 - 本文重点研究了一个有趣的任务，即无监督跨语料库语音情感识别（SER），其中标记的训练（源）语料库和未标记的测试（目标）语料库具有不同的特征分布，导致源域和目标域之间存在差异。针对这一问题，该文提出一种融合Transformers和互信息（MI）的跨语料库SER的无监督域适配方法。最初，我们的方法使用 Transformer 的编码器层从提取的段级 log-Mel 频谱图特征中捕获话语中的长期时间动态，从而为两个域中的每个话语生成相应的话语级特征。然后，提出了一种基于混合MaxMin MI策略的无监督特征分解方法，从提取的混合话语级特征中分别学习域不变特征和域特定特征，尽可能消除两个域之间的差异，同时保留各自的特征。最后，设计了一种交互式多头注意力融合策略来学习领域不变特征和领域特定特征之间的互补性，以便它们可以交互融合为SER。 在IEMOCAP和MSP-Improv数据集上的大量实验证明了我们提出的方法在无监督跨语料库SER任务上的有效性，优于最先进的无监督跨语料库SER方法。

### Feeling Without Sharing: A Federated Video Emotion Recognition Framework Via Privacy-Agnostic Hybrid Aggregation
 - [[paper](https://dl.acm.org/doi/10.1145/3503161.3548278)]
 - 视频数据的爆炸式增长为情感识别带来了新的机遇和挑战。视频情感应用具有巨大的商业价值，但可能涉及非法窥探个人情感，引发了隐私保护方面的争议。联邦学习 （FL） 范式可以从根本上解决公众对视频情感识别中数据隐私日益增长的担忧。然而，由于任务的独特性，传统的 FL 方法表现不佳：由于情感标签偏差和跨文化表达差异，数据在客户之间是异质的。为了减少异构数据，我们提出了EmoFed，这是一个通过多组聚类和与隐私无关的混合聚合进行基于视频的联邦学习情感识别的实用框架。它在保护隐私的同时生成了一个通用适用和改进的模型，该模型在群体感知个性化聚合下训练本地模型。为了进一步鼓励在客户之间交流全面且与隐私无关的信息，我们将全局层和个性化层的模型参数上传到服务器。我们将同态加密方法用于个性化层，由于在加密/解密过程中不会向模型更新添加噪声，因此不会造成学习准确性损失。该方法适用于基于视频的情绪识别任务，以预测演员的情绪表达和观众诱导的情绪。对四个基准的大量实验和消融研究证明了我们方法的有效性和实用性。

### EASE: Robust Facial Expression Recognition via Emotion Ambiguity-SEnsitive Cooperative Networks
 - [[paper](https://dl.acm.org/doi/10.1145/3503161.3548005)]
 - 面部表情识别 （FER） 在实际应用中起着至关重要的作用。然而，在野外收集的大规模FER数据集通常包含噪声。更重要的是，由于情感的模糊性，具有多种情绪的面部图像很难与带有嘈杂标签的面部图像区分开来。因此，为 FER 训练一个鲁棒的模型是具有挑战性的。为了解决这个问题，我们提出了包含两个组件的情感模糊-SEnsitive 合作网络（EASE）。首先，歧义敏感学习模块将训练样本分为三组。两个网络中损失较小的样本被视为干净样本，损失较大的样本则为噪声样本。请注意，对于一个网络与另一个网络不一致的冲突样本，我们使用情绪的极性线索将传达模棱两可情绪的样本与带有噪音的样本区分开来。在这里，我们利用KL发散来优化网络，使它们能够关注非主导情绪。EASE的第二部分旨在加强合作网络的多样性。随着培训周期的增加，合作网络将趋同为共识。我们根据特征之间的相关性构造了一个惩罚项，这有助于网络从图像中学习不同的表征。对 6 个流行的面部表情数据集进行的广泛实验表明，EASE 优于最先进的方法。

### Disentangled Representation Learning for Multimodal Emotion Recognition
 - [[paper](https://dl.acm.org/doi/10.1145/3503161.3547754)]
 - 多模态情感识别旨在从文本、音频和视觉模态中识别人类情感。以前的方法要么探索不同模态之间的相关性，要么设计复杂的融合策略。然而，严重的问题是，分布差距和信息冗余往往存在于异构模态之间，导致学习到的多模态表示可能未被提炼。基于这些观察结果，我们提出了一种特征解缠多模态情感识别（FDMER）方法，该方法学习每种模态的公共和私人特征表示。具体来说，我们设计了通用编码器和专用编码器，以将每个模态分别投影到模态不变和模态特定子空间中。模态不变子空间旨在探索不同模态之间的共性，充分缩小分布差距。特定于模态的子空间试图增强多样性并捕获每个模态的独特特征。之后，引入模态判别器，以对抗方式指导公共编码器和私有编码器的参数学习。我们通过为上述子空间设计量身定制的损耗来实现模态一致性和视差约束。此外，我们提出了一个跨模态注意力融合模块来学习自适应权重以获得有效的多模态表示。最终表示用于不同的下游任务。实验结果表明，FDMER在两个多模态情感识别基准上都优于最先进的方法。此外，通过对多模态体液检测任务的实验，进一步验证了模型的有效性。

### Dilated Context Integrated Network with Cross-Modal Consensus for Temporal Emotion Localization in Videos
 - [[paper](https://dl.acm.org/doi/10.1145/3503161.3547886)]
 - [[code](https://github.com/YYJMJC/Temporal-Emotion-Localization-inVideos)]
 - 理解人类情感是智能机器人提供更好的人机交互的关键能力。现有作品局限于经过剪辑的视频级情感分类，未能找到与情感对应的时间窗口。在本文中，我们介绍了一项名为视频时间情感定位（TEL）的新任务，该任务旨在检测人类情感，并在带有对齐字幕的未修剪视频中定位其相应的时间边界。与时间动作定位相比，TEL提出了三个独特的挑战：1）情绪具有极其不同的时间动态;2）情感线索既嵌入在表象中，也嵌入在复杂的情节中;3）细粒度的时间标注复杂且费力。为了解决前两个挑战，我们提出了一种具有粗细双流架构的新型扩张上下文集成网络。粗流通过对多粒度时间上下文进行建模来捕获不同的时间动态。细流通过推理粗流的多粒度时间上下文之间的依赖关系，自适应地将其集成到细粒度的视频片段特征中，从而实现复杂情节的理解。为了应对第三个挑战，我们引入了一种跨模态共识学习范式，该范式利用对齐视频和字幕之间的固有语义共识来实现弱监督学习。我们贡献了一个新的测试集，其中包含 3,000 个手动注释的时间边界，以便可以定量评估未来对 TEL 问题的研究。大量实验表明，我们的方法在时间情感定位方面是有效的。

### Dilated Context Integrated Network with Cross-Modal Consensus for Temporal Emotion Localization in Videos
 - [[paper](https://dl.acm.org/doi/10.1145/3503161.3547886)]
 - [[code](https://github.com/YYJMJC/Temporal-Emotion-Localization-inVideos)]
 - 理解人类情感是智能机器人提供更好的人机交互的关键能力。现有作品局限于经过剪辑的视频级情感分类，未能找到与情感对应的时间窗口。在本文中，我们介绍了一项名为视频时间情感定位（TEL）的新任务，该任务旨在检测人类情感，并在带有对齐字幕的未修剪视频中定位其相应的时间边界。与时间动作定位相比，TEL提出了三个独特的挑战：1）情绪具有极其不同的时间动态;2）情感线索既嵌入在表象中，也嵌入在复杂的情节中;3）细粒度的时间标注复杂且费力。为了解决前两个挑战，我们提出了一种具有粗细双流架构的新型扩张上下文集成网络。粗流通过对多粒度时间上下文进行建模来捕获不同的时间动态。细流通过推理粗流的多粒度时间上下文之间的依赖关系，自适应地将其集成到细粒度的视频片段特征中，从而实现复杂情节的理解。为了应对第三个挑战，我们引入了一种跨模态共识学习范式，该范式利用对齐视频和字幕之间的固有语义共识来实现弱监督学习。我们贡献了一个新的测试集，其中包含 3,000 个手动注释的时间边界，以便可以定量评估未来对 TEL 问题的研究。大量实验表明，我们的方法在时间情感定位方面是有效的。
