# [Publications in ACM MM 2023](https://www.acmmm2023.org/)

## Emotion-Related
### StyleEDL: Style-Guided High-order Attention Network for Image Emotion Distribution Learning
 - [[paper](https://dl.acm.org/doi/pdf/10.1145/3581783.3612040)]
 - [[code](https://github.com/liuxianyi/StyleEDL)]
 - 随着通过图像表达情感的趋势，情感分布学习越来越受到关注。 对于人类主观性引起的情感模糊性，先前的大量方法通常集中于从图像的整体或重要部分中学习适当的表示。 然而，他们很少考虑与风格信息建立联系，尽管这可以更好地理解图像。 在本文中，我们提出了一种用于图像情感分布学习的风格引导高阶注意网络，称为 StyleEDL，它通过探索视觉内容的层次风格信息来交互地学习图像的风格感知表示。 具体来说，我们考虑探索基于 GRAM 的风格表示之间的层内和层间相关性，同时利用对手约束的高阶注意力机制来捕获微妙视觉部分之间的潜在交互。 此外，我们引入了文体图卷积网络来动态生成内容相关的情感表示，以有利于最终的情感分布学习。 在几个基准数据集上进行的广泛实验证明了我们提出的 StyleEDL 与最先进的方法相比的有效性。

### ASTDF-Net: Attention-Based Spatial-Temporal Dual-Stream Fusion Network for EEG-Based Emotion Recognition
 - [[paper](https://dl.acm.org/doi/pdf/10.1145/3581783.3612208)]
 - [code暂无]
 - 基于脑电图（EEG）的情绪识别在情感计算和人机交互领域引起了极大的关注并取得了长足的进步。 然而，大多数现有研究忽略了脑电信号中复杂时空模式的耦合和互补性。 此外，如何利用和融合高冗余和低信噪比脑电信号中的关键判别方面仍然是情感识别的巨大挑战。 在本文中，我们提出了一种新颖的基于注意力的时空双流融合网络，名为 ASTDF-Net，用于基于脑电图的情感识别。 具体来说，ASTDF-Net 包括三个主要阶段：首先，协作嵌入模块被设计为学习联合潜在子空间，以捕获 EEG 信号中复杂时空信息的耦合。 其次，采用堆叠的并行空间和时间注意力流来提取最基本的判别特征并过滤掉与任务无关的冗余因素。 最后，提出了基于混合注意力的特征融合模块，集成从双流结构中发现的重要特征，以充分利用不同特征的互补性。 对两个公开可用的情绪识别数据集的广泛实验表明，我们提出的方法始终优于最先进的方法。

### Emotion-Prior Awareness Network for Emotional Video Captioning
 - [[paper](https://dl.acm.org/doi/pdf/10.1145/3581783.3611726)]
 - [[code](https://github.com/songpipi/EPAN)]
 - 情感视频字幕（EVC）是一项新兴任务，用视频中表达的内在情感来描述事实内容。 在字幕生成阶段有效感知微妙且模糊的视觉情感线索对于 EVC 任务至关重要。 然而，现有的字幕方法通常忽视了对用户生成视频中情感的学习，从而使得生成的句子有点无聊和没有灵魂。为了解决这个问题，本文以类人感知优先的方式提出了一种新的情感字幕视角，即首先感知固有情感，然后利用感知到的情感线索来支持字幕生成。 具体来说，我们设计了一个情绪优先意识网络（EPAN）。 它主要受益于新颖的树形结构情感学习模块，涉及目录级心理类别和词汇级常用词，以达到明确且细粒度的情感感知的目标。 此外，我们在目录级别和词汇级别之间开发了一种新颖的从属情感掩蔽机制，有助于从粗到细的情感学习。 之后，在情感优先的情况下，我们可以通过利用视觉、文本和情感语义的互补来有效地解码情感描述。 此外，我们还引入了三个简单而有效的优化目标，可以从情感字幕、分层情感分类和情感对比学习的角度显着促进情感学习。 三个基准数据集上的充分实验结果清楚地证明了我们提出的 EPAN 在语义和情感指标方面相对于现有 SOTA 方法的优势。 广泛的消融研究和可视化分析进一步揭示了我们的情感视频字幕方法的良好可解释性。

### Feeling Positive? Predicting Emotional Image Similarity from Brain Signals
 - [[paper](https://dl.acm.org/doi/pdf/10.1145/3581783.3613442)]
 - [code暂无]
 - 目前的视觉相似性概念基于从图像内容导出的特征。 这忽略了用户对内容的情感体验，以及用户搜索图像时的感受。 在这里，我们将效价（情感评估的积极或消极量化）视为图像相似性的一个新维度。 我们报告了最大的神经影像实验，该实验通过使用脑机接口的功能性近红外光谱来量化和预测视觉内容的效价。 我们表明，情感相似性可以 (1)~直接从响应视觉刺激的大脑信号中解码，(2)~用于预测情感图像相似性，平均准确度为 0.58，对于高唤醒刺激的准确度为 0.65，以及 (3)~有效地用于补充基于内容的模型的情感相似性估计； 例如，当融合 fNIRS 和图像排名时，检索 F-measure@20 为 0.70。 我们的工作为情感多媒体分析、检索和用户建模开辟了新的研究途径。

### Multimodal Physiological Signals Fusion for Online Emotion Recognition
 - [[paper](https://dl.acm.org/doi/pdf/10.1145/3581783.3612555)]
 - [code暂无]
 - 由于情绪的复杂性和生理信号的个体差异，基于多模态生理的情绪识别是最可用但最具挑战性的研究之一。 然而，现有研究主要结合多模态数据来融合离线场景中的多模态信息，忽略了在线场景中多模态数据之间的数据/模态相关性以及非平稳生理信号的个体差异。 在本文中，我们提出了一种新颖的在线多模态超图学习（OMHGL）方法来融合多模态信息，以基于时间序列生理信号进行情感识别。 我们的方法包括多模态超图融合和在线超图学习。 具体来说，多模态超图融合可以融合多模态生理信号，通过利用多模态信息和多模态数据/模态之间的高阶相关性，有效地获得情感依赖信息。 在线超图学习旨在通过更新超图投影从在线数据中学习新信息。 因此，当目标数据以在线方式到达时，所提出的在线情感识别模型可以更有效地对目标对象进行情感识别。 实验结果表明，所提出的方法显着优于基线，并与在线情感识别任务中最先进的方法进行了比较。

### Revisiting Disentanglement and Fusion on Modality and Context in Conversational Multimodal Emotion Recognition
 - [[paper](https://dl.acm.org/doi/pdf/10.1145/3581783.3612053)]
 - [code暂无]
 - 让机器在对话场景下理解多模态环境中的人类情感一直是一个热门研究课题，其任务是对话中的多模态情感分析（MM-ERC）。 近年来，MM-ERC 受到了一致的关注，人们提出了多种方法来确保更好的任务性能。 大多数现有工作将 MM-ERC 视为标准的多模态分类问题，并执行多模态特征解缠和融合以最大化特征效用。 然而，在重新审视 MM-ERC 的特征之后，我们认为在特征解缠和融合步骤中，应该同时对特征多模态和会话情境化进行正确建模。 在这项工作中，我们的目标是通过充分考虑上述见解来进一步推动任务绩效。 一方面，在特征解耦过程中，基于对比学习技术，我们设计了一种双级解耦机制（DDM），将特征解耦到模态空间和话语空间。 另一方面，在特征融合阶段，我们分别提出了用于多模态和上下文集成的贡献感知融合机制（CFM）和上下文拒绝机制（CRM）。 他们共同安排多模式和上下文功能的正确集成。 具体来说，CFM 显式地动态管理多模态特征贡献，而 CRM 则灵活地协调对话上下文的引入。 在两个公共 MM-ERC 数据集上，我们的系统始终如一地实现了最先进的性能。 进一步的分析表明，我们提出的所有机制通过自适应地充分利用多模态和上下文特征，极大地促进了 MM-ERC 任务。 请注意，我们提出的方法具有促进更广泛的其他会话多模式任务的巨大潜力。

### Unlocking the Power of Multimodal Learning for Emotion Recognition in Conversation
 - [[paper](https://dl.acm.org/doi/pdf/10.1145/3581783.3613846)]
 - [code暂无]
 - 对话中的情绪识别旨在识别每个话语背后的情绪，它在各个领域都具有巨大的潜力。 人类对情绪的感知依赖于多种方式，例如语言、声调和面部表情。 虽然许多研究已经纳入多模态信息来增强情绪识别，但当添加额外模态时，多模态模型的性能通常会趋于稳定。 我们通过实验证明，这种平台期的主要原因是跨模态梯度分配不平衡。 为了解决这个问题，我们提出了细粒度自适应梯度调制，这是一种重新平衡模态梯度的插件方法。 实验结果表明，我们的方法提高了所有基线模型的性能，并且优于现有的插件方法。

### Emo-DNA: Emotion Decoupling and Alignment Learning for Cross-Corpus Speech Emotion Recognition
 - [[paper](https://dl.acm.org/doi/pdf/10.1145/3581783.3611704)]
 - [[code](https://github.com/Jiaxin-Ye/Emo-DNA)]
 - 跨语料库语音情感识别（SER）旨在将从标记良好的语料库推断语音情感的能力推广到未标记的语料库，由于两个语料库之间存在显着差异，这是一项相当具有挑战性的任务。 现有的方法通常基于无监督域适应（UDA），很难通过全局分布对齐来学习语料库不变的特征，但不幸的是，所得的特征与语料库特定的特征混合在一起，或者没有类别区分性。 为了应对这些挑战，我们提出了一种用于跨语料库 SER 的新型情感解耦和对齐学习框架（EMO-DNA），这是一种学习情感相关语料库不变特征的新型 UDA 方法。 EMO-DNA 的新颖之处有两个：对比情感解耦和双层情感对齐。 一方面，我们的对比情感解耦通过对比解耦损失实现解耦学习，以加强情感相关特征与语料库特定特征的可分离性。 另一方面，我们的双级情感对齐引入了自适应阈值伪标签来选择可信目标样本进行类级对齐，并执行语料库级对齐来共同指导模型学习跨语料库的类判别性语料库不变特征 。 大量的实验结果证明，在多个跨语料库场景中，EMO-DNA 的性能优于最先进的方法。

### Graph to Grid: Learning Deep Representations for Multimodal Emotion Recognition
 - [[paper](https://dl.acm.org/doi/pdf/10.1145/3581783.3612074)]
 - [[code](https://github.com/Jinminbox/G2G)]
 - 基于脑电图（EEG）和补偿生理信号（例如眼球追踪）的多模态情绪识别在抑郁症的诊断和康复跟踪中显示出潜力。 由于多通道脑电图信号通常被处理为一维（1-D）图形特征，现有方法只能采用不发达的浅层模型来识别情绪。 然而，这些简单的模型由于其表示能力有限，难以解耦复杂的情绪模式。 为了解决这个问题，我们提出了图到网格（G2G），这是一种简洁且即插即用的模块，可将一维图状数据转换为二维（2-D）网格状数据 通过数字关系编码。 之后，成熟的深度模型，例如 ResNet，可以用于下游任务。 此外，G2G将之前复杂的多模态融合简化为输入矩阵增广操作，大大降低了模型设计和参数调优的难度。 三个公共数据集（SEED、SEED5 和 MPED）上的广泛结果表明，所提出的方法在单模态和多模态设置中实现了最先进的情感识别准确性，并具有良好的跨会话泛化能力。 G2G能够开发更合适的多模态情感识别算法以用于后续研究。

### Multimodal Prompt Transformer with Hybrid Contrastive Learning for Emotion Recognition in Conversation
 - [[paper](https://dl.acm.org/doi/pdf/10.1145/3581783.3611805)]
 - [code暂无]
 - 对话中的情绪识别（ERC）在推动人机交互的发展中发挥着重要作用。 情感可以以多种模态存在，多模态ERC主要面临两个问题：（1）跨模态信息融合过程中的噪声问题，（2）语义相似但类别不同的样本较少的情感标签的预测问题。 为了解决这些问题并充分利用各模态的特征，我们采取了以下策略：首先，对表征能力强的模态进行深层情感线索提取，对表征能力较弱的模态设计特征过滤器作为多模态提示信息。 然后，我们设计了多模态提示变换器（MPT）来执行跨模态信息融合。 MPT将多模态融合信息嵌入到Transformer的每个注意力层中，让提示信息参与编码文本特征并与多层次文本信息融合，以获得更好的多模态融合特征。 最后，我们使用混合对比学习（HCL）策略来优化模型处理少量样本标签的能力。 该策略利用无监督对比学习来提高多模态融合的表示能力，利用监督对比学习来挖掘少量样本的标签信息。 实验结果表明，我们提出的模型在两个基准数据集上优于 ERC 中最先进的模型。

### Progressive Visual Content Understanding Network for Image Emotion Classification
 - [[paper](https://dl.acm.org/doi/pdf/10.1145/3581783.3612186)]
 - [code暂无]
 - 大多数现有的图像情感分类方法直接从单个情感标签监督的图像中提取特征。 然而，这种方法有一个局限性，即情感差距，它限制了这些功能的能力，因为它们并不总是与用户感知的情绪一致。 为了有效地弥合情感差距，本文提出了一种受人类阶段性情感感知过程启发的视觉内容理解网络。 所提出的网络由三个感知模块组成，旨在提取多级信息。 首先，实体感知模块从图像中提取实体。 其次，属性感知模块提取每个实体的属性内容。 第三，情感感知模块根据实体和属性信息提取情感特征。 我们通过图像分割和视觉语言模型生成实体和属性的伪标签，为网络学习提供辅助指导。 渐进的实体和属性理解使网络能够分层提取语义级特征进行情感分析。 大量的实验表明，我们的渐进式学习网络在图像情感分类的各种基准数据集上实现了卓越的性能。

### Multi-label Emotion Analysis in Conversation via Multimodal Knowledge Distillation
 - [[paper](https://dl.acm.org/doi/pdf/10.1145/3581783.3612517)]
 - [code暂无]
 - 评估对话中说话者的情绪对于需要人机交互的各种应用至关重要。 然而，多种情绪状态同时出现（例如，“愤怒”和“沮丧”可能同时出现，或者其中一种情绪可能影响另一种情绪的发生），并且由于说话者的内在因素（例如，他们的个性化行为的影响），它们的动态演变可能会发生巨大变化。 社会文化教育和人口背景）和外部环境。 到目前为止，之前的重点是仅评估说话者在给定时间观察到的主导情绪，这很容易在测试过程中为困难的多标签产生误导性的分类决策。 在这项工作中，我们通过高效的多模态变压器网络提出了自监督多标签同伴协作蒸馏（SeMuL-PCD）学习，其中来自多个特定模式同伴网络（例如转录、音频、视觉）的补充反馈被提炼成 用于同时估计多种情绪的单模式集成融合网络。 所提出的多模态蒸馏损失通过最小化与对等网络的 Kullback-Leibler 散度来校准融合网络。 此外，每个对等网络都使用自我监督的对比目标进行调节，以提高不同社会人口说话者背景的泛化能力。 通过启用同伴协作学习，允许每个网络独立学习其特定于模式的判别模式，SeMUL-PCD 在不同的对话环境中都是有效的。 特别是，该模型不仅在多个大型公共数据集（例如 MOSEI、EmoReact 和 ElderReact）上优于当前最先进的模型，而且在跨数据集中的加权 F1 分数提高了约 17% 实验设置。 该模型还展示了跨年龄和人口统计多样化人群的令人印象深刻的泛化能力。

### EmotionKD: A Cross-Modal Knowledge Distillation Framework for Emotion Recognition Based on Physiological Signals
 - [[paper](https://dl.acm.org/doi/pdf/10.1145/3581783.3612277)]
 - [[code](https://github.com/YuChengLiu-Alex/EmotionKD)]
 - 使用多模态生理信号的情感识别是情感计算的一个新兴领域，与单模态方法相比，它显着提高了性能。 脑电图 (EEG) 和皮肤电反应 (GSR) 信号的结合对于客观和互补的情绪识别特别有效。 然而，EEG信号采集的高成本和不便严重阻碍了多模态情感识别在现实场景中的普及，而GSR信号更容易获得。 为了应对这一挑战，我们提出了 EmotionKD，这是一种跨模态知识蒸馏的框架，可以在统一的框架下同时对 GSR 和 EEG 信号的异质性和交互性进行建模。 通过使用知识蒸馏，可以将完全融合的多模态特征转移到单模态 GSR 模型以提高性能。 此外，提出了自适应反馈机制，使多模态模型能够在知识蒸馏过程中根据单模态模型的性能进行动态调整，从而引导单模态模型增强其在情感识别方面的性能。 我们的实验结果表明，所提出的模型在两个公共数据集上实现了最先进的性能。 此外，我们的方法有可能减少对多模态数据的依赖，同时降低性能损失，使情感识别更加适用和可行。


# Contrastive Learning
 - [SCLAV: Supervised Cross-modal Contrastive Learning for Audio-Visual Coding](https://dl.acm.org/doi/abs/10.1145/3581783.3613805)
 - [CONVERT:Contrastive Graph Clustering with Reliable Augmentation](https://arxiv.org/pdf/2308.08963.pdf)
 - [DealMVC: Dual Contrastive Calibration for Multi-view Clustering](https://arxiv.org/pdf/2308.09000.pdf)
 - [Entropy Neural Estimation for Graph Contrastive Learning](https://arxiv.org/pdf/2307.13944.pdf)
 - [GCL: Gradient-Guided Contrastive Learning for Medical Image Segmentation with Multi-Perspective Meta Labels](https://arxiv.org/pdf/2309.08888.pdf)
 - [Calibration-based Dual Prototypical Contrastive Learning Approach for Domain Generalization Semantic Segmentation](https://arxiv.org/pdf/2309.14282)
 - [Interpolation Normalization for Contrast Domain Generalization](https://dl.acm.org/doi/abs/10.1145/3581783.3611841)
 - [Triple-Granularity Contrastive Learning for Deep Multi-View Subspace Clustering](https://dl.acm.org/doi/abs/10.1145/3581783.3611844)
 - [Exploring Universal Principles for Graph Contrastive Learning: A Statistical Perspective](https://dl.acm.org/doi/abs/10.1145/3581783.3612229)
 - [Self-Contrastive Graph Diffusion Network](https://arxiv.org/pdf/2307.14613)
 - [AesCLIP: Multi-Attribute Contrastive Learning for Image Aesthetics Assessment](https://dl.acm.org/doi/abs/10.1145/3581783.3611969)
 - [Hierarchical Reasoning Network with Contrastive Learning for Few-Shot Human-Object Interaction Recognition](https://dl.acm.org/doi/abs/10.1145/3581783.3612311)
 - [Feature-Suppressed Contrast for Self-Supervised Food Pre-training](https://arxiv.org/pdf/2308.03272.pdf)
 - [Zero-Shot Object Detection by Semantics-Aware DETR with Adaptive Contrastive Loss](https://dl.acm.org/doi/abs/10.1145/3581783.3612523)
 - [Fine-Grained Spatiotemporal Motion Alignment for Contrastive Video Representation Learning](https://arxiv.org/pdf/2309.00297.pdf)
 - [CONICA: A Contrastive Image Captioning Framework with Robust Similarity Learning](https://dl.acm.org/doi/abs/10.1145/3581783.3612326)
 - [Contrast-augmented Diffusion Model with Fine-grained Sequence Alignment for Markup-to-Image Generation](https://arxiv.org/pdf/2308.01147)
 - [Graph Spectral Perturbation for 3D Point Cloud Contrastive Learning](https://dl.acm.org/doi/abs/10.1145/3581783.3612469)
 - [Confidence-Aware Contrastive Learning for Semantic Segmentation](https://dl.acm.org/doi/abs/10.1145/3581783.3612037)
 - [Semi-supervised Domain Adaptation via Joint Contrastive Learning with Sensitivity](https://dl.acm.org/doi/abs/10.1145/3581783.3611991)
 - [Cross-modal contrastive learning for multimodal fake news detection](https://dl.acm.org/doi/pdf/10.1145/3581783.3613850)
 - [Relational Contrastive Learning for Scene Text Recognition](https://arxiv.org/pdf/2308.00508.pdf)
 - [Multimodal Prompt Transformer with Hybrid Contrastive Learning for Emotion Recognition in Conversation](https://arxiv.org/pdf/2310.04456.pdf)
 - [Auditory Attention Decoding with Task-Related Multi-View Contrastive Learning](https://dl.acm.org/doi/pdf/10.1145/3581783.3611869)
 - [Towards Adaptable Graph Representation Learning: An Adaptive Multi-Graph Contrastive Transformer](https://dl.acm.org/doi/pdf/10.1145/3581783.3612358)
 - [Efficient Labelling of Affective Video Datasets via Few-Shot & Multi-Task Contrastive Learning](https://arxiv.org/pdf/2308.02173.pdf)
 - [Contrastive Intra- and Inter-Modality Generation for Enhancing Incomplete Multimedia Recommendation](https://www.cs.emory.edu/~jyang71/files/ci2mg.pdf)
 - [AdvCLIP: Downstream-agnostic Adversarial Examples in Multimodal Contrastive Learning](https://arxiv.org/pdf/2308.07026.pdf)
 - [A Contrastive Learning Framework for Dual-Target Cross-Domain Recommendation](https://dl.acm.org/doi/abs/10.1145/3581783.3612250)
 - [Unsupervised Hashing with Contrastive Learning by Exploiting Similarity Knowledge and Hidden Structure of Data](https://dl.acm.org/doi/abs/10.1145/3581783.3612596)
 - [Modal-aware Bias Constrained Contrastive Learning for Multimodal Recommendation](https://dl.acm.org/doi/abs/10.1145/3581783.3612568) 

# Weakly-supervised Learning
 - [Weakly-Supervised Text Instance Segmentation](https://arxiv.org/pdf/2303.10848.pdf)
 - [FDCNet: Feature Drift Compensation Network for Class-Incremental Weakly Supervised Object Localization](https://arxiv.org/pdf/2309.09122.pdf)
 - [Exploring Dual Representations in Large-Scale Point Clouds: A Simple Weakly Supervised Semantic Segmentation Framework](https://dl.acm.org/doi/abs/10.1145/3581783.3612224)
 - [Dynamic Contrastive Learning with Pseudo-samples Intervention for Weakly Supervised Joint Video MR and HD](https://dl.acm.org/doi/pdf/10.1145/3581783.3612384)
 - [Weakly-supervised Video Scene Graph Generation via Unbiased Cross-modal Learning](https://dl.acm.org/doi/abs/10.1145/3581783.3612019)
 - [Topological Structure Learning for Weakly-Supervised Out-of-Distribution Detection](https://arxiv.org/pdf/2209.07837)
 - [Rethinking the Localization in Weakly Supervised Object Localization](https://arxiv.org/pdf/2308.06161.pdf)
 - [QA-CLIMS: Question-Answer Cross Language Image Matching for Weakly Supervised Semantic Segmentation](https://dl.acm.org/doi/abs/10.1145/3581783.3612148)
 - [LocLoc: Low-level Cues and Local-area Guides for Weakly Supervised Object Localization](https://dl.acm.org/doi/abs/10.1145/3581783.3612165)
 - [Counterfactual Cross-modality Reasoning for Weakly Supervised Video Moment Localization](https://arxiv.org/pdf/2308.05648.pdf)
   
# Label Noise Learning
 - [PNT-Edge: Towards Robust Edge Detection with Noisy Labels by Learning Pixel-level Noise Transitions](https://arxiv.org/pdf/2307.14070.pdf)
 - [Multi-teacher Self-training for Semi-supervised Node Classification with Noisy Labels](https://dl.acm.org/doi/abs/10.1145/3581783.3613117)
 - [ALEX: Towards Effective Graph Transfer Learning with Noisy Labels](https://arxiv.org/pdf/2309.14673.pdf)
 - [Adaptive Contrastive Learning for Learning Robust Representations under Label Noise]()
 - [ROAD: Robust Unsupervised Domain Adaptation with Noisy Labels](https://dl.acm.org/doi/abs/10.1145/3581783.3612296)

